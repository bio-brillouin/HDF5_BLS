\documentclass{article}
\usepackage{tikz}
\usetikzlibrary{positioning}

\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30]
\tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{file} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=gray!30]
\tikzstyle{arrow} = [thick,->,>=stealth]    

\title{Project presentation}
\author{Pierre Bouvet}
\date{\today}

\begin{document}

\maketitle


\section{Introduction}
This document is made as to present the vision of the HDF\_BLS library and the software approach for the unification of all BLS data. This file is meant to be evolutive with the different needs encountered by researchers. It however presents succintly the current vision of the project and allows to better understand its development and its parts.

\section{HDF5 file structure}

    \subsection{Multiple measures of same nature}
        The vision of the project is to unify all BLS data into a single HDF5 file. This file will be used to store all the data and metadata of the BLS. The data will be stored in a hierarchical structure, all the data related to BLS being stored the "Data" group. This choice allows results from other techinques to be stored in the same file, only in other, independent groups, to minimize the risk of nomenclature competition between techinques. The "Data" group will contain all the data. These data can either be arranged in groups, whose identifiers will be "Data\_i" or directly as numerical arrays. We propose to only allow one measure per group or file, and to call the array corresponding to the raw data recovered from the spectrometer "Raw\_data". For multiple measures, we propose to create multiple groups, following this structure:

        \begin{verbatim}
            file.h5
            +-- Data
            |   +-- Data_1
            |   |   +-- Raw_data
            |   +-- Data_2
            |   |   +-- Raw_data
            |   +-- ...
        \end{verbatim}

    \subsection{Attributes}

        The attributes will follow a hierarchical structure. The attributes that apply to all "Data\_i" will be stored in the "Data" group, while the parameters that apply to a specific "Data\_i" will be stored in the "Data\_i" group. All the attributes are stored as text. Attributes are then divided in three categories:
        \begin{itemize}
            \item Attributes that are specific to the spectrometer used, such as the wavelength of the laser, the type of laser, the type of detector, etc. These attributes are recognized by the capital letter word "SPECTROMETER" in the name of the attribute.
            \item Attributes that are specific to the sample, such as the date of the measurement, the name of the sample, etc. These attributes are recognized by the capital letter word "MEASURE" in the name of the attribute.
            \item Attributes that are specific to the original file format, such as the name of the file, the date of the file, the version of the file, the precision used on the storage of the data, etc. These attributes are recognized by the capital letter word "FILEPROP" in the name of the attribute.
        \end{itemize}

        The name of the attribues contains the unit of the attribute if it has units, in the shape of an underscore followed by the unit in parenthesis. Some parameters that can be represented by a series of norms will also be defined in a given norm, such as the ISO8601 for the date. These norms are however not specified in the name of the attribute. Here are some examples of attributes:
        \begin{itemize}
            \item "SPECTROMETER.Detector\_Type" is the type of the detector used.
            \item "MEASURE.Sample" is the name of the sample.
            \item "MEASURE.Exposure\_(s)" is the exposire of the sample given in seconds
            \item "MEASURE.Date\_of\_measurement" is the date of the measurement. 
            \item "FILEPROP.Name" is the name of the file.
        \end{itemize}

        To unify the name of attributes, we will create and maintain a spreadsheet containing all the attributes and their units. This spreadsheet will be updated as new attributes are added to the project and defined with a version number that will also be stored in the attribtutes of each data attributes. This spreadsheet is meant to be exported in a CSV file that can be used to update the attributes of the data.

    \subsection{Meta-files}
        It can be useful to store in a same file, measures coming from different instruments, taken in different conditions, or that we just want to separate from other groups of measures. In that end, we propose a tree-like structure of the HDF5 file, where each group can contain sub-groups, which can also contain sub-groups etc. In order to unify the way we access these groups, we propose to identify them by a unique identifier of the form "Data\_i", where "i" is an integer. Here is an example of a meta-file:

        \begin{verbatim}
            file.h5
            +-- Data
            |   +-- Data_1
            |   |   +-- Data_1
            |   |   |   +-- Raw_data
            |   |   +-- Data_2
            |   |   |   +-- Raw_data
            |   |   +-- ...
            |   +-- Data_2
            |   |   +-- Data_1
            |   |   |   +-- Data_1
            |   |   |   |   +-- Raw_data
            |   |   |   +-- Data_2
            |   |   |   |   +-- Raw_data
            |   |   |   +-- ...
            |   |   +-- Data_2
            |   |   |   +-- Raw_data
            |   +-- ...
        \end{verbatim}

\section{Pipelines}
   
    \subsection{General view} 

        The goal of this software is to go from a storaged data file to a usable data set with a reproducible, stable and unified treatment protocol. To schematize this treatment protocol, we propose the following diagram:

        \begin{center}
        \begin{tikzpicture}[node distance=2cm]

        \node (start) [startstop] {Start};
        \node (acquire) [process, below of=start] {Raw Data Acquisition};
        \node (psd) [process, below of=acquire, align=center] {Obtaining the \\ Power Spectrum Density};
        \node (fits) [process, below of=psd] {Fitting of the PSD};
        \node (end) [startstop, below of=fits] {End};

        \draw [arrow] (start) -- (acquire);
        \draw [arrow] (acquire) -- (psd);
        \draw [arrow] (psd) -- (fits);
        \draw [arrow] (fits) -- (end);

        \node (filestart) [file, right of=start, xshift=3cm] {Create HDF5 file};
        \node (fileraw) [file, below of=filestart, align=center] {Add raw data \\ to the file};
        \node (filepsd) [file, below of=fileraw, align=center] {Store the \\ Power Spectrum Density\\and the channel freauencies};
        \node (filetreat) [file, below of=filepsd, align=center] {Store the \\ Treated data};
        \node (fileend) [file, below of=filetreat] {Save file};

        \draw [arrow] (filestart) -- (fileraw);
        \draw [arrow] (fileraw) -- (filepsd);
        \draw [arrow] (filepsd) -- (filetreat);
        \draw [arrow] (filetreat) -- (fileend);

        \end{tikzpicture}
        \end{center}

    \subsection{Conceptual issues and envisionned solutions}
    
        \subsubsection{Raw Data Acquisition}
            Raw data will take a variety of form, and will sometimes need to be stored in combination with other data (e.g. array of time points). As such, we propose to store the raw data and all of the data it depends on for its understanding in the same group, naming "Raw Data" the array containing the measure of the energy of the signal, and "Abscissa\_i" all the other arrays that are used to understand the data. The resulting will therefore be the following:

            \begin{verbatim}
                file.h5
                +-- Data
                |   +-- Data_1
                |   |   +-- Raw_data
                |   |   +-- Abscissa_1
                |   |   +-- Abscissa_2
                |   |   +-- ...
                |   +-- ...
            \end{verbatim}

            It is imprecindible therefore to store the technique used to acquire the Brillouin spectrum in the attribute of the file. This information will be stored in the "SPECTROMETER.Type" attribute. 
    
        \subsubsection{Conversion of the data to a Power Spectrum Density}

            This is arguably the most important and controversial step of the pipeline. The goal of this step is to obtain the Power Spectrum Density of the data. Many different techniques can be used to do so, most of which will vary from laboratory to laboratory, and relying either on theoretical knowledge of the instrument or data calibration. As it is still unclear how to best convert the data to a PSD, we let the user choose the method they want to use. We propose to store the technique used in the "SPECTROMETER.PSD\_Method" attribute, following the following nomenclature: "Laboratory\_Method\_version". For example, the following attributes could be used:

            \begin{verbatim}
                SPECTROMETER.PSD_Method = "MedUniWien_1VIPA_1.0"
                SPECTROMETER.PSD_Method = "EMBL_2VIPA_0.5"
                SPECTROMETER.PSD_Method = "CellSense_CS363_2.5"
            \end{verbatim}

            Note that this conversion can be performed using an independent and even confidential pipeline, allowing firms to use this format without compromising their proprietary data. In the future, it might however be interesting to have a unified pipeline for all the techniques, which could automatically be adjusted to the "SPECTROMETER.Type" attribute. The end goal of this step is to obtain two arrays: 
            \begin{itemize}
                \item The "PSD" array, containing the PSD of the data.
                \item The "Channel\_frequencies" array, containing the channel frequencies of the data.
            \end{itemize}

            At this stage, the raw data might become irrelevant and costly in terms of storage space. We can therefore delete the "Raw\_data" array and the "Abscissa\_i" arrays associated to it at this stage. The resulting file would then take the following form:

            \begin{verbatim}
                file.h5
                +-- Data
                |   +-- Data_1
                |   |   +-- PSD
                |   |   +-- Channel_frequencies
                |   |   +-- ...
                |   +-- ...
            \end{verbatim}

            Alternatively, if different treatments are to be compared, we recommend creating different groups. The resulting file would then take the following form:

            \begin{verbatim}
                file.h5
                +-- Data
                |   +-- Data_1
                |   |   +-- Raw_data (optional)
                |   |   +-- Abscissa_1 (optional)
                |   |   +-- Data_1
                |   |   |   +-- PSD
                |   |   |   +-- Channel_frequencies
                |   |   +-- Data_2
                |   |   |   +-- PSD
                |   |   |   +-- Channel_frequencies
                |   |   +-- ...
                |   +-- ...
            \end{verbatim}

        \subsection{Fitting of the PSD}

            The fit of the Power Spectrum Density (PSD) can be unified in a single function that can accept parameters to precise the fit. We encourage you to refer to the dedicated notebook: \textbf{Treatment\_pipeline} found in the \textbf{guides} folder of the directory.

            The treatment should result in a new group "Treat\_i" composed of at least four arrays:
            \begin{itemize}
                \item The "Shift" array, containing the values of the fitted frequency shifts
                \item The "Linewidth" array, containing the values of the fitted linewidths
                \item The "Shift\_var" arrays, containing the variance of the fitted frequency shifts, used to auqntify the uncertainty of the fitted values
                \item The "Linewidth\_var" arrays, containing the variance of the fitted linewidths, used to auqntify the uncertainty of the fitted values
            \end{itemize}

            Additionnaly, other arrays can be added to the "Treat\_i" group, such as the "SNR" and "Amplitude" arrays, or any other relevant information extracted from the PSD.

            This treated group should in any case have as attributes all the parameters used in the treatment as well as the different steps followed for the treatment. 

            The returned structure should therefore be of the form:
            \begin{verbatim}
                file.h5
                +-- Data
                |   +-- Data_1
                |   |   +-- PSD
                |   |   +-- Channel_frequencies
                |   |   +-- Treat_1
                |   |   |   +-- Shift
                |   |   |   +-- Linewidth
                |   |   |   +-- Shift_var
                |   |   |   +-- Linewidth_var
                |   |   |   +-- ...
                |   +-- ...
            \end{verbatim}

            Other treatments on the same PSD can with this architecture be stored in the same file, by changing the index of the "Treat\_i" group. For example, if two peaks are to be fitted independently, the resulting file would take the following form:

            \begin{verbatim}
                file.h5
                +-- Data
                |   +-- Data_1
                |   |   +-- PSD
                |   |   +-- Channel_frequencies
                |   |   +-- Treat_1
                |   |   |   +-- Shift
                |   |   |   +-- Linewidth
                |   |   |   +-- Shift_var
                |   |   |   +-- Linewidth_var
                |   |   |   +-- ...
                |   |   +-- Treat_2
                |   |   |   +-- Shift
                |   |   |   +-- Linewidth
                |   |   |   +-- Shift_var
                |   |   |   +-- Linewidth_var
                |   |   |   +-- ...
                |   +-- ...
            \end{verbatim}

\section{Library implementation}

    \section{Package structure}
            
        The easiest way to implement the described structure is to create a package that is both easy to use and reliable. This package is to be seen as the back-end of the project: where the handling of data is described and implemented.



\end{document}